---
layout: post
title:  "AutoRec ë…¼ë¬¸ ë¦¬ë·°, PyTorchë¡œ êµ¬í˜„í•˜ê¸°"
date:   2021-02-03
categories: ["2021","ai"]
update:   2021-02-03
use_math: true
comment: true
tags: [ai,recommend]
---



ë‚´ìš©ì´ ì›Œë‚™ ê°„ë‹¨í•˜ê³  ê¸€ë„ ì§§ì•„ì„œ ë¦¬ë·°ëŠ” ëª¨ë¸ê¹Œì§€ë§Œ ê°„ë‹¨íˆ í•˜ê³  ë°”ë¡œ êµ¬í˜„ìœ¼ë¡œ ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤.



## AutoRec: Autoencoders Meet Collaborative Filtering

### ABSTRACT

ì´ ë…¼ë¬¸ì—ì„œëŠ” í˜‘ì—…í•„í„°ë§(ì´í•˜ CF)ë¥¼ ìœ„í•œ autoencoder í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì €ìì˜ ê²½í—˜ì— ë”°ë¥´ë©´ **AutoRec**ì˜ ëª¨ë¸ì€ ì‘ê³  íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•˜ê³  Movielens ë°ì´í„°ì…‹ê³¼ Netflix ë°ì´í„°ì…‹ì— ëŒ€í•´ ë‹¹ì‹œ state-of-the-art CF ê¸°ìˆ ì´ì—ˆë˜ biased MF, RBM-CF, LLORMAë³´ë‹¤ ì›”ë“±í•œ ì„±ëŠ¥ì„ ëƒˆë‹¤ê³  í•˜ë„¤ìš”.

### ğŸ‘€ What is Autoencoder ?

ì¸íŠ¸ë¡œì—ì„œ ì„¤ëª…í–ˆë“¯ì´ ì´ ëª¨ë¸ì—ì„œëŠ” autoencoderë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ autoencoderê°€ ëŒ€ì²´ ë­˜ê¹Œìš”?

>ì˜¤í†  ì¸ì½”ë”ëŠ” ê°ë…ë˜ì§€ ì•Šì€ ë°©ì‹ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ë°ì´í„° ì½”ë”©ì„ í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì¸ê³µ ì‹ ê²½ë§ì˜ í•œ ìœ í˜•ì…ë‹ˆë‹¤. ì˜¤í†  ì¸ì½”ë”ì˜ ëª©ì ì€ ë„¤íŠ¸ì›Œí¬ê°€ ì‹ í˜¸ "ë…¸ì´ì¦ˆ"ë¥¼ ë¬´ì‹œí•˜ë„ë¡ í›ˆë ¨í•¨ìœ¼ë¡œì¨ ì¼ë°˜ì ìœ¼ë¡œ ì°¨ì› ê°ì†Œë¥¼ ìœ„í•œ ë°ì´í„° ì…‹ì— ëŒ€í•œ í‘œí˜„ì„ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

*ì¶œì²˜ : [ìœ„í‚¤í”¼ë””ì•„](https://en.wikipedia.org/wiki/Autoencoder)*

ì…ë ¥ì—ì„œ ì¶œë ¥ìœ¼ë¡œ ë³µì‚¬í•˜ëŠ” ì‹ ê²½ë§ì¸ë°, ì¤‘ê°„ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ í•´ì„œ ë‹¨ìˆœ ë³µì‚¬í•˜ì§€ ëª»í•˜ë„ë¡ ë§‰ê³  ì´ëŸ° ê³¼ì •ë“¤ì„ í†µí•´ì„œ ë°ì´í„°ì…‹ì„ íš¨ìœ¨ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤. 

![image](https://user-images.githubusercontent.com/51329156/106574235-00e19300-657e-11eb-8a99-c5192cd8cabd.png)

ê·¸ë¦¼ì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´ ì˜¤í† ì¸ì½”ë”ëŠ” í•­ìƒ ì¸ì½”ë”ì™€ ë””ì½”ë”, ë‘ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

- **ì¸ì½”ë”(encoder)** : ì¸ì§€ ë„¤íŠ¸ì›Œí¬(recognition network)ë¼ê³ ë„ í•˜ë©°, ì…ë ¥ì„ ë‚´ë¶€ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

- **ë””ì½”ë”(decoder)** : ìƒì„± ë„¤íŠ¸ì›Œí¬(generative nework)ë¼ê³ ë„ í•˜ë©°, ë‚´ë¶€ í‘œí˜„ì„ ì¶œë ¥ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ì…ë ¥ê³¼ ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ìˆ˜ê°€ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒë§Œ ì œì™¸í•˜ë©´ ì¼ë°˜ì ì¸ MLPì™€ ë™ì¼í•œ êµ¬ì¡°ì¸ ê²ƒ ë˜í•œ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. **Loss function**ì€ ì…ë ¥ê³¼ ì¬êµ¬ì„±ëœ ì…ë ¥ê°’, ì¦‰ ì¶œë ¥ì˜ ì°¨ì´ë¥¼ ê°€ì§€ê³  ê³„ì‚°í•©ë‹ˆë‹¤. 

ì •ë¦¬í•´ë³´ë©´ ì˜¤í† ì¸ì½”ë”ë¥¼ í†µí•´ ì…ë ¥ê°’ì„ **ì¬êµ¬ì„±(reconstruction)**í•˜ê³  ì…ë ¥ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ íŠ¹ì„±ë“¤ì„ í•™ìŠµí•˜ëŠ” ì¹œêµ¬ì…ë‹ˆë‹¤. ì˜¤í† ì¸ì½”ë”ë„ ì¢…ë¥˜ê°€ ë§ì€ë°, í›„ì— í•„ìš”í•  ë•Œ ì¶”ê°€ì ìœ¼ë¡œ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

**ì°¸ê³ **

> ğŸš€ [ì˜¤í† ì¸ì½”ë” (AutoEncoder)](https://excelsior-cjh.tistory.com/187)

### THE AUTOREC MODEL

CFì—ì„œ mëª…ì˜ ì‚¬ìš©ìì™€, nê°œì˜ ì•„ì´í…œì— ëŒ€í•´ rating matrix $R \in \mathbb{R}^{m \times n}$ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ê° User uëŠ” itemì— ëŒ€í•œ í‰ê°€ ë²¡í„° $r^{(u)} = (R_{u1},R_{u2},...,R_{un})$ë¥¼ ê°€ì§€ê³  Item iëŠ” ë§ˆì°¬ê°€ì§€ë¡œ $r^{(i)} = (R_{1i},R_{2i},...,R_{mi})$ë¡œ í‘œí˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ì•ìœ¼ë¡œì˜ ì„¤ëª…ì€ Item-basedë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ê² ìŠµë‹ˆë‹¤.  ì´ ëª¨ë¸ì—ì„œ í•˜ë ¤ëŠ” ì¼ì€ ì›ë˜ ì…ë ¥ê°’ $r^{(i)}$ì™€ ì˜¤í† ì¸ì½”ë”ë¥¼ í†µí•´ ì¬êµ¬ì„±í•œ ì…ë ¥ê°’ ê°„ì˜ ì˜¤ì°¨ë¥¼ êµ¬í•´ ê·¸ ì˜¤ì°¨ë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤. ì…ë ¥ì„ ì¬êµ¬ì„±í•œ ê°’, ì¦‰ ì˜¤í† ì¸ì½”ë”ë¥¼ ê±°ì¹œ outputì€ ìˆ˜ì‹ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

$$h(r;\theta) = f(W \\cdot g(Vr + \mu) + b)$$

$ f,g $ëŠ” ê°ê° í™œì„±í™” í•¨ìˆ˜ì´ê³ , íŒŒë¼ë¯¸í„° ê°ê°ì˜ ì‚¬ì´ì¦ˆëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

$W \\in R^{d \times k} ,V \in R^{k \times d}, \mu \in R^{k},b \in R^{d}$

ì—¬ê¸°ì„œ $d$ëŠ” m í˜¹ì€ nì´ê³  $k$ëŠ” hidden_layerì˜ feature ìˆ˜ ì…ë‹ˆë‹¤. 

Objective funtionì€ L2 lossë¥¼ ì†ì‹¤í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ëŠ” L2 Regularizationì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/51329156/106887861-1d6dfe80-6729-11eb-9037-79926af7e822.png)


$\left \|\cdot\right \|_{O}^{2}$ì˜ ì˜ë¯¸ëŠ” ê´€ì¸¡ëœ rating, ì¦‰ interaction matrixì—ì„œ ë¹„ì–´ìˆì§€ ì•Šì€ ìš”ì†Œì— ëŒ€í•´ì„œë§Œ ê³ ë ¤í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. MFì™€ëŠ” ë‹¬ë¦¬ $g(\cdot)$ì—ì„œ non-linear í•œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ latent vectorë¥¼ ë” ì˜ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

## PyTorchë¡œ êµ¬í˜„

### ğŸ“– Dataset

ì „ì²´ ë°ì´í„°ì…‹ì„ train, test ì…‹ìœ¼ë¡œ ë‚˜ëˆ„ê³  `__getitem__()`ì—ì„œëŠ” ê° itemì— ëŒ€í•œ í‰ê°€ ë²¡í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

``` python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class MovielensDataset(Dataset):
    def __init__(self, path, index, columns, values, train_ratio=.9, test_ratio=.1, train = None):
        super().__init__()
        # self.index = index
        # self.columns = columns
        # self.values = values
        self.rating_df = pd.read_csv(path)
        self.inter_matrix = self.rating_df.pivot(index=index, columns=columns, values=values).fillna(0).to_numpy()

        self.train_ratio = train_ratio
        self.test_ratio = test_ratio

        self.total_indices = np.arange(len(self.inter_matrix))
        self.test_indices = np.random.choice(self.total_indices, size=(int(len(self.inter_matrix) * self.test_ratio),), replace=False)
        self.train_indices = np.array(list(set(self.total_indices)-set(self.test_indices)))

        if train != None:
            if train == True:
                self.inter_matrix = self.inter_matrix[self.train_indices]
            elif train == False:
                self.inter_matrix = self.inter_matrix[self.test_indices] 
    def __len__(self):
        return len(self.inter_matrix)
    def __getitem__(self, index: int):
        """
        get rating vector of one item(or user) : [0., 0., 4., 3., ..., 0., 0.]
        """
        return torch.Tensor(self.inter_matrix[index]).to(device)
```

### ğŸ“– Model

ì˜¤í† ì¸ì½”ë”ëŠ” **encoder**ì™€ **decoder**ë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤ê³  í–ˆìœ¼ë‹ˆ ëª¨ë¸ì—ì„œë„ ë˜‘ê°™ì´ êµ¬í˜„í•´ì¤ë‹ˆë‹¤. ìœ„ì˜ ìˆ˜ì‹ì—ì„œëŠ” $h(\cdot)$ì— í•´ë‹¹í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.

``` python
class AutoRec(nn.Module):
    """
    input -> hidden -> output(output.shape == input.shape)
    encoder: input -> hidden
    decoder: hidden -> output
    """
    def __init__(self, n_users, n_items, n_factors=200):
        super(AutoRec, self).__init__()
        self.n_factors = n_factors
        self.n_users = n_users
        self.n_items = n_items

        self.encoder = nn.Sequential(
            nn.Linear(self.n_users,self.n_factors),
            nn.Sigmoid(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(self.n_factors, self.n_users),
            nn.Identity(),
        )

    def forward(self, x):
        return self.decoder(self.encoder(x)).to(device)
```

`Sigmoid()`ì™€ `Identity()`ëŠ” ë…¼ë¬¸ì—ì„œ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ ì¡°í•©ì´ì—ˆìŠµë‹ˆë‹¤.

### ğŸ“– Criterion

ë…¼ë¬¸ì—ì„œ ê´€ì¸¡ëœ ratingì— ëŒ€í•´ì„œë§Œ ì˜¤ì°¨ë¥¼ ê²€ì‚¬í•œë‹¤ê³  í–ˆìœ¼ë‹ˆ ë¹„ì–´ìˆëŠ” ê°’ì€ ë¹¼ê³  ê³„ì‚°í•˜ê¸° ìœ„í•´ ê¸°ì¡´ì— ì‚¬ìš©í•˜ë˜ RMSE í´ë˜ìŠ¤ì— ì•½ê°„ ìˆ˜ì •ì„ í–ˆìŠµë‹ˆë‹¤. ì œê°€ í•œ ë°©ë²• ë§ê³ ë„ numpyì˜ `nonzero()`ë¥¼ ì‚¬ìš©í•´ë„ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

``` python
class MRMSELoss(nn.Module):
    """
    MaskedRootMSELoss() uses only observed ratings.
    According to docs(https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html), 
    'mean' is set by default for 'reduction' and can be avoided by 'reduction="sum"'
    """
    def __init__(self, reduction='sum'):
        super().__init__()
        self.reduction = reduction
        self.mse = nn.MSELoss(reduction=self.reduction).to(device)
    def forward(self, pred, rating):
        mask = rating != 0
        masked_pred = pred * mask.float()
        num_observed = torch.sum(mask).to(device) if self.reduction == 'mean' else torch.Tensor([1.]).to(device)
        loss = torch.sqrt(self.mse(masked_pred, rating) / num_observed)
        return loss
```

Objective functionì—ì„œ $\lambda$ ëŠ” Optimizerì— `weight_decay`ê°’ìœ¼ë¡œ ì£¼ë©´ ë©ë‹ˆë‹¤.

ì‹¤í–‰ì½”ë“œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

``` python
import torch
from torch import nn, optim,cuda
from models import AutoRec, MRMSELoss
from data import MovielensDataset
from torch.utils.data import DataLoader, Dataset
import argparse
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""
AutoRec: Autoencoders Meet Collaborative Filtering implementation with PyTorch
"""
# Setting
def check_positive(val):
    val = int(val)
    if val <=0:
        raise argparse.ArgumentError(f'{val} is invalid value. epochs should be positive integer')
    return val

parser = argparse.ArgumentParser(description='AutoRec with PyTorch')
parser.add_argument('--epochs', '-e', type=check_positive, default=30)
parser.add_argument('--batch', '-b', type=check_positive, default=32)
parser.add_argument('--lr', '-l', type=float, help='learning rate', default=1e-3)
parser.add_argument('--wd', '-w', type=float, help='weight decay(lambda)', default=1e-2)
parser.add_argument('--ksize', '-k', type=check_positive, help='hidden layer feature_size', default=200)

args = parser.parse_args()

path = 'data/movielens/'
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
n_users, n_items = (9724, 610)

# Dataset & Dataloader

train_dataset = MovielensDataset(path=os.path.join(path,'ratings.csv'), index='movieId', columns='userId', train=True)
test_dataset = MovielensDataset(path=os.path.join(path,'ratings.csv'), index='movieId', columns='userId', train=False)

train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch,shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=args.batch,shuffle=True)

# Model & Criterion
model = AutoRec(n_users=n_users, n_items=n_items, n_factors=200).to(device)

# Criterion & Optimizer
criterion = MRMSELoss().to(device)
optimizer = optim.Adam(model.parameters(), weight_decay= args.wd, lr=args.lr)

# Train & Test
def train(epoch):
    process = []
    for idx, (data,) in enumerate(train_loader):
        optimizer.zero_grad()
        pred = model(data)
        loss = criterion(pred,data)
        loss.backward()
        optim.step()
        process.append(loss.item())
        if idx % 100 == 0:
            print (f"[+] Epoch {epoch} [{idx * args.batch} / {len(train_loader.dataset)}] - RMSE {sum(process) / len(process)}")
    return torch.Tensor(sum(process) / len(process)).to(device)
def test():
    process = []
    for idx, (data,) in enumerate(test_loader):
        optimizer.zero_grad()
        pred = model(data)
        loss = criterion(pred,data)
        loss.backward()
        optim.step()
        process.append(loss.item())
    print (f"[*] Test RMSE {sum(process) / len(process)} ")
    return torch.Tensor(sum(process) / len(process)).to(device)
    
# Run
if __name__=="__main__":
    train_rmse = torch.Tensor([]).to(device)
    test_rmse = torch.Tensor([]).to(device)
    for epoch in range(args.epochs):
        train_rmse = torch.cat((train_rmse, train(epoch)), dim=0)
        test_rmse = torch.cat((test_rmse, test()), dim=0)
    plt.plot(range(args.epochs),train_rmse, range(args.epochs),test_rmse)
    plt.xlabel('epoch')
    plt.ylabel('RMSE')
    plt.xticks(range(0,args.epochs,2))
    plt.show()
```

### Result

![autoRec](https://user-images.githubusercontent.com/51329156/106747396-357d4980-6667-11eb-9b75-796767ce92de.png)

ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ê²°ê³¼ë§Œí¼ì€ ë‚´ë ¤ê°€ì§€ ì•ŠëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.. ê·¸ëƒ¥ í•œ ë²ˆ êµ¬í˜„í•´ë³¸ ê±°ì— ëŒ€í•´ ì˜ì˜ë¥¼ ê°€ì ¸ì•¼ í•  ê²ƒ ê°™ì•„ìš”.

### Conclusion

ë…¼ë¬¸ë„ ì§§ê³  ëª¨ë¸ë„ ë‹¨ìˆœí•´ì„œ êµ¬í˜„ì´ ìµìˆ™í•˜ì‹œì§€ ì•Šì€ ë¶„ë“¤ë„ ì‰½ê²Œ í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì½”ë“œ êµ¬í˜„ ì‹œ ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹œë©´ ë©”ì¼ ì£¼ì„¸ìš”. ê°ì‚¬í•©ë‹ˆë‹¤ !